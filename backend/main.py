from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Dict, Any, Optional
import requests
import json
import os

app = FastAPI(title="Model Audit Backend")

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

LLAMA_SERVER_URL = "http://127.0.0.1:8080/completion"
USE_MOCK = os.getenv("USE_MOCK", "False").lower() == "true"

class AnalysisRequest(BaseModel):
    auc: Optional[float] = None
    accuracy: Optional[float] = None
    precision: Optional[float] = None
    recall: Optional[float] = None
    f1Score: Optional[float] = None
    ece: Optional[float] = None
    brier: Optional[float] = None
    drift: Optional[float] = None
    missingRate: Optional[float] = None
    labelShift: Optional[float] = None
    positiveRate: Optional[float] = None
    dataIntegrity: Optional[float] = None
    
    class Config:
        extra = "allow"

@app.get("/health")
def health_check():
    # Check if llama server is reachable
    if not USE_MOCK:
        try:
            requests.get(f"{LLAMA_SERVER_URL.replace('/completion', '/health')}", timeout=1)
        except:
            return {"status": "backend_online_but_model_server_down"}
    return {"status": "online"}

@app.post("/analyze")
def analyze(request: Dict[str, Any]):
    print(f"Received metrics: {request}")
    
    if USE_MOCK:
        print("Returning MOCK response")
        return {
            "label": "Potential Drift Detected (MOCK)",
            "explanation": "This is a MOCK response. (Generated by Mock Mode)"
        }

    try:
        # Construct prompt
        metrics_str = "\n".join([f"- {k}: {v}" for k, v in request.items() if v is not None and v != ""])
        
        prompt = f"""<|system|>
You are an expert AI model auditor. precise, critical, and thorough.
Analyze the following model performance metrics and determine if there is a drift or issue.

Metrics:
{metrics_str}

Provide your analysis in the following JSON format:
{{
    "label": "Status Label (e.g., No Drift, Major Drift, Critical Failure)",
    "explanation": "A comprehensive and detailed explanation of why you assigned this label. Analyze specific metrics that are concerning, explain the potential impact of the drift/issue, and suggest possible causes or next steps. Aim for at least 3-4 sentences."
}}
<|end|>
<|assistant|>
"""

        payload = {
            "prompt": prompt,
            "n_predict": 300,
            "temperature": 0.1,
            "json_schema": {
                "type": "object",
                "properties": {
                    "label": {"type": "string"},
                    "explanation": {"type": "string"}
                },
                "required": ["label", "explanation"]
            }
        }

        print("Sending request to llama.cpp server...")
        response = requests.post(LLAMA_SERVER_URL, json=payload, timeout=60)
        
        if response.status_code == 200:
            result = response.json()
            # llama.cpp returns 'content' containing the generated string
            content = result.get('content', '')
            print(f"Model response: {content}")
            try:
                # content should be JSON because we enforced grammar/schema (if supported by server version)
                # or just plain text that looks like JSON.
                return json.loads(content)
            except:
                return {
                    "label": "Analysis Generated",
                    "explanation": content
                }
        else:
            raise HTTPException(status_code=500, detail=f"Model server error: {response.text}")

    except Exception as e:
        print(f"Error during analysis: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
